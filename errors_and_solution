1. Helm Upgrade Argument Error

Error: Error: "helm upgrade" requires 2 arguments    
Description: This error occurs because the helm upgrade command mandates the release name and chart path as the first two arguments in the specified order. While the necessary parameters may be present in the command, their positioning is incorrect.   
Resolution: Modify the installation command to ensure the release name and chart path are correctly positioned as the primary arguments. Use the following command structure:
Bash

helm upgrade karpenter-release oci://public.ecr.aws/karpenter/karpenter \
--install \
--version "1.3.3" \
--namespace "kube-system" \
--create-namespace \
--set "settings.clusterName=my-cluster" \
--set "settings.interruptionQueue=my-cluster" \ # Note: Consider removal if causing issues (See Section 4)
--set controller.resources.requests.cpu=1 \
--timeout 10m \
--set controller.resources.requests.memory=1Gi \
--set controller.resources.limits.cpu=1 \
--set controller.resources.limits.memory=1Gi \
--set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="<controller-role-arn>" \
--wait
  
2. Context Deadline Exceeded Error

Error: Error: context deadline exceeded    
Description: This error typically indicates that a Kubernetes operation, such as the Helm installation for Karpenter, has surpassed the allocated timeout period. Potential causes include network latency, insufficient cluster resources, or prolonged provisioning times.   
Resolution:
Increase the operation timeout using the --timeout flag in the Helm command (e.g., --timeout 15m).   
Verify cluster connectivity and health using kubectl get nodes and kubectl cluster-info.   
Ensure the EKS cluster possesses adequate resources (kubectl describe nodes | grep -A 5 "Allocated resources").   
Check for pods stuck in a pending state across all namespaces (kubectl get pods --all-namespaces | grep Pending).   
Review events within the kube-system namespace for potential indicators (kubectl get events -n kube-system).   
Examine Helm history for previous failed installation attempts (helm history karpenter -n kube-system).  

3. Resource Already Exists Error (PodDisruptionBudget Conflict)

Error: Error: rendered manifests contain a resource that already exists. Unable to continue with install: PodDisruptionBudget "karpenter" in namespace "kube-system" exists... invalid ownership metadata... annotation validation error: key "meta.helm.sh/release-name" must equal "karpenter-release": current value is "karpenter"    
Description: This error signifies a naming conflict. A PodDisruptionBudget (PDB) resource named "karpenter" already exists within the kube-system namespace, likely created by a different or previous Helm release. Helm cannot import this resource due to mismatched ownership metadata.   
Resolution:
Identify the existing Karpenter installation using Helm commands (e.g., helm list -n kube-system).   
If the previous installation is obsolete, uninstall it completely: helm uninstall karpenter -n kube-system.   
Manually delete the conflicting PDB resource: kubectl delete pdb karpenter -n kube-system. 

4. AWS STS AssumeRoleWithWebIdentity Error / SQS NonExistentQueue Error

Error Log Snippets:
{"level":"ERROR",..."message":"ec2 api connectivity check failed",..."error":"...operation error STS: AssumeRoleWithWebIdentity, https response error StatusCode: 403...api error AccessDenied: Not authorized to perform sts:AssumeRoleWithWebIdentity"}
panic: operation error SQS: GetQueueUrl, https response error StatusCode: 400,...AWS.SimpleQueueService.NonExistentQueue: The specified queue does not exist.    
Description: These errors indicate issues with AWS permissions or resource configuration. The first suggests the Karpenter service account lacks the necessary permissions to assume the specified IAM role via web identity. The second (panic) points to a failure in locating the SQS queue defined for interruption handling, often because the queue name provided (settings.interruptionQueue during Helm install) is incorrect or the queue does not exist.   
Resolution:
STS Error: Verify the IAM Role ARN and the associated Trust Policy/Permissions. Ensure the OIDC provider is correctly configured in IAM and the trust relationship allows the sts:AssumeRoleWithWebIdentity action from the Karpenter service account.
SQS Error: Remove the --set "settings.interruptionQueue=..." parameter from the Helm installation command. Karpenter can operate without explicit interruption queue handling, relying on other mechanisms. Re-running the installation without this setting often resolves the panic. The recommended final installation command might look like this:
Bash

helm install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.3.3 --namespace kube-system \
--set serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn=<controller_role_arn> \
--set settings.clusterName=my-cluster \
--set controller.resources.requests.cpu=1 \
--set controller.resources.requests.memory=1Gi \
--set controller.resources.limits.cpu=1 \
--set controller.resources.limits.memory=1Gi \
--wait
  
5. Invalid EC2NodeClass AMI Selector Error

Error: The EC2NodeClass "default" is invalid: spec.amiSelectorTerms[0].alias: Invalid value: "string": 'alias' is improperly formatted, must match the format 'family@version'    
Controller Log: {"level":"ERROR",..."message":"Reconciler error",..."error":"getting amis, getting AMI queries, failed to discover AMIs for alias \"al2023@1.0.0\""}
Description: This validation error occurs when the amiSelectorTerms within an EC2NodeClass specification contains improperly formatted values. The alias field requires a specific format, typically family@version (e.g., al2023@latest).   
Resolution: Correct the value provided in the amiSelectorTerms.alias field within the EC2NodeClass definition to match the required family@version format. Example: al2023@latest. 

6. Subnet Discovery Error (Missing Tag)

Controller Log: {"level":"ERROR",..."message":"failed listing instance types for default",..."error":"no subnets found"}    
Description: This error indicates that Karpenter could not identify suitable subnets for node provisioning. This typically happens when the required discovery tag (karpenter.sh/discovery: <cluster-name>) is missing from the subnet resources in AWS.   
Resolution: Ensure that the subnets intended for use by Karpenter are tagged correctly in AWS with the key karpenter.sh/discovery and the value set to your cluster's name (e.g., my-cluster).   

7. NodePool Not Ready Error

Controller Log: {"level":"ERROR",..."message":"ignoring nodepool, not ready",..."NodePool":{"name":"default"}}    
Description: This log message suggests the controller is currently ignoring the specified NodePool (e.g., "default") because it is not yet in a ready state. This could be due to ongoing reconciliation, configuration issues within the NodePool or its associated NodeClass, or dependencies not being met.
Resolution: Investigate the status and configuration of the "default" NodePool and its related EC2NodeClass. Check for preceding errors in the controller logs related to this NodePool or its dependencies (like subnet or AMI issues described above). Ensure all referenced resources (IAM roles, security groups, subnets, AMIs) are correctly configured and accessible.
